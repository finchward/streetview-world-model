{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbbad82",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d16810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name = \"Version_one\"\n",
    "    load_model = False\n",
    "    loaded_model = \"v2\"\n",
    "    loaded_checkpoint = \"abc\"\n",
    "    \n",
    "\n",
    "    sample_every_x_batches = 48 #avoid being divisible by latent_persistence_turns\n",
    "    inference_samples = 300\n",
    "    inference_step_size = 1\n",
    "    img_dir = 'images'\n",
    "\n",
    "    model_resolution = (384, 512)\n",
    "    features = [64, 128, 256, 512, 1024] \n",
    "    #64 -> 128 -> 256 -> 512 -> 1024/16 ->| 2048/16 |  -> 1024/32 append 2048/32 -> 1024/32 -> 512/64 -> 256/128 -> 128/256 -> 64/512\n",
    "    #256 -> 128 -> 64 -> 32 -> 16 |  192 -> 96 -> 48 -> 24 -> 12\n",
    "    \n",
    "    latent_persistence_turns = 5\n",
    "    predictions_per_image = 1\n",
    "\n",
    "    time_embedding_dim = 512 #Must be even\n",
    "    movement_embedding_dim = 512\n",
    "    latent_dimension = 1024\n",
    "    max_batches = 1000\n",
    "    rotation_probability = 0.6\n",
    "    initial_pages = [ \n",
    "        r\"https://www.google.com/maps/@-38.5922817,176.8199381,3a,75y,271.3h,104.26t/data=!3m7!1e1!3m5!1s1Pt-bx9x0-vdMyd-R05xZw!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fcb_client%3Dmaps_sv.tactile%26w%3D900%26h%3D600%26pitch%3D-14.260361780245958%26panoid%3D1Pt-bx9x0-vdMyd-R05xZw%26yaw%3D271.30228279891134!7i13312!8i6656?entry=ttu&g_ep=EgoyMDI1MDkwNy4wIKXMDSoASAFQAw%3D%3D\",\n",
    "        #r\"https://www.google.com/maps/place/Pukaki+Canal+Road,+Canterbury+Region+7999/@-44.3213197,170.0447441,3a,75y,195.92h,96.45t/data=!3m7!1e1!3m5!1sI9B-vhHlUJpLWYoqiLarOw!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fcb_client%3Dmaps_sv.tactile%26w%3D900%26h%3D600%26pitch%3D-6.447370168190574%26panoid%3DI9B-vhHlUJpLWYoqiLarOw%26yaw%3D195.9163922970516!7i16384!8i8192!4m6!3m5!1s0x6d2ae1d22564538b:0xac6cf5a0c84835c2!8m2!3d-44.2098275!4d170.0758422!16s%2Fg%2F11h6yd4mrn?entry=ttu&g_ep=EgoyMDI1MDkwNy4wIKXMDSoASAFQAw%3D%3D\",\n",
    "       # r\"https://www.google.com/maps/@-45.1338186,168.7592437,3a,75y,211.88h,90t/data=!3m7!1e1!3m5!1slN5kBn2x9rltM8rmRvc6iw!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fcb_client%3Dmaps_sv.tactile%26w%3D900%26h%3D600%26pitch%3D0%26panoid%3DlN5kBn2x9rltM8rmRvc6iw%26yaw%3D211.87823!7i13312!8i6656?entry=ttu&g_ep=EgoyMDI1MDkwNy4wIKXMDSoASAFQAw%3D%3D\"\n",
    "    ] # make one of these huia\n",
    "\n",
    "    learning_rate = 4e-5\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    graph_update_freq = 1\n",
    "    recent_losses_shown = 1500\n",
    "    loss_bucket_size = 10\n",
    "    save_freq = 100\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c409f4",
   "metadata": {},
   "source": [
    "## grapher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91bc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Grapher:\n",
    "    def __init__(self):\n",
    "        plt.ion()\n",
    "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "       \n",
    "        # Data storage for graph 1\n",
    "        self.train_x_1 = []\n",
    "        self.train_y_1 = []\n",
    "        self.val_x_1 = []\n",
    "        self.val_y_1 = []\n",
    "        self.recon_val_x_1 = []\n",
    "        self.recon_val_y_1 = []\n",
    "        self.div_val_x_1 = []\n",
    "        self.div_val_y_1 = []\n",
    "        \n",
    "        # Data storage for graph 2\n",
    "        self.train_x_2 = []\n",
    "        self.train_y_2 = []\n",
    "        self.val_x_2 = []\n",
    "        self.val_y_2 = []\n",
    "        self.recon_val_x_2 = []\n",
    "        self.recon_val_y_2 = []\n",
    "        self.div_val_x_2 = []\n",
    "        self.div_val_y_2 = []\n",
    "       \n",
    "        # Line objects for graph 1\n",
    "        self.train_line_1, = self.ax1.plot([], [], 'r-', label='Training', linewidth=1)\n",
    "        self.val_line_1, = self.ax1.plot([], [], 'b-', label='Validation')\n",
    "        self.recon_val_line_1, = self.ax1.plot([], [], 'g-', label='Reconstruction Val', linewidth=1)\n",
    "        self.div_val_line_1, = self.ax1.plot([], [], 'm-', label='Divergence Val', linewidth=1)\n",
    "        \n",
    "        # Line objects for graph 2\n",
    "        self.train_line_2, = self.ax2.plot([], [], 'r-', label='Training', linewidth=1)\n",
    "        self.val_line_2, = self.ax2.plot([], [], 'b-', label='Validation')\n",
    "        self.recon_val_line_2, = self.ax2.plot([], [], 'g-', label='Reconstruction Val', linewidth=1)\n",
    "        self.div_val_line_2, = self.ax2.plot([], [], 'm-', label='Divergence Val', linewidth=1)\n",
    "        \n",
    "        self.ax1.set_yscale(\"log\")\n",
    "        self.ax2.set_yscale(\"log\")\n",
    "     \n",
    "        self.ax1.legend()\n",
    "        self.ax2.legend()\n",
    "       \n",
    "    def update_line(self, graph_index, line_type, x_data, y_data):\n",
    "        if graph_index == 0:\n",
    "            if line_type == \"Training\":\n",
    "                self.train_x_1 = x_data\n",
    "                self.train_y_1 = y_data\n",
    "                self.train_line_1.set_data(x_data, y_data)\n",
    "            elif line_type == \"Validation\":\n",
    "                self.val_x_1 = x_data\n",
    "                self.val_y_1 = y_data\n",
    "                self.val_line_1.set_data(x_data, y_data)\n",
    "            elif line_type == \"Reconstruction loss\":\n",
    "                self.recon_val_x_1 = x_data\n",
    "                self.recon_val_y_1 = y_data\n",
    "                self.recon_val_line_1.set_data(x_data, y_data)\n",
    "            elif line_type == \"Divergence loss\":\n",
    "                self.div_val_x_1 = x_data\n",
    "                self.div_val_y_1 = y_data\n",
    "                self.div_val_line_1.set_data(x_data, y_data)\n",
    "            self.ax1.relim()\n",
    "            self.ax1.autoscale_view()\n",
    "        else:\n",
    "            if line_type == \"Training\":\n",
    "                self.train_x_2 = x_data\n",
    "                self.train_y_2 = y_data\n",
    "                self.train_line_2.set_data(x_data, y_data)\n",
    "            elif line_type == \"Validation\":\n",
    "                self.val_x_2 = x_data\n",
    "                self.val_y_2 = y_data\n",
    "                self.val_line_2.set_data(x_data, y_data)\n",
    "            elif line_type == \"Reconstruction Val\":\n",
    "                self.recon_val_x_2 = x_data\n",
    "                self.recon_val_y_2 = y_data\n",
    "                self.recon_val_line_2.set_data(x_data, y_data)\n",
    "            elif line_type == \"Divergence Val\":\n",
    "                self.div_val_x_2 = x_data\n",
    "                self.div_val_y_2 = y_data\n",
    "                self.div_val_line_2.set_data(x_data, y_data)\n",
    "            self.ax2.relim()\n",
    "            self.ax2.autoscale_view()\n",
    "       \n",
    "        plt.draw()\n",
    "        plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c196f74",
   "metadata": {},
   "source": [
    "## inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ecdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "fig, ax, im = None, None, None\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_next_img(model, device, sample_name, prev_img, movement, latent, next_img=None):\n",
    "    model.eval()\n",
    "    base_img = prev_img.clone()\n",
    "    global fig, ax, im\n",
    "    if fig is None:\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(np.zeros((Config.model_resolution[0], Config.model_resolution[1], 3), dtype=np.float32))\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    out_img = None\n",
    "    pbar = tqdm.tqdm(range(Config.inference_samples), desc=f'Sampling')\n",
    "    for time_step in pbar:\n",
    "        time_step = torch.tensor([time_step]).to(device)\n",
    "        dx = Config.inference_step_size / Config.inference_samples\n",
    "        delta = model.predict_delta(prev_img, time_step, movement, latent)\n",
    "        prev_img += delta * dx\n",
    "        prev_img = torch.clamp(prev_img, 0, 1)\n",
    "\n",
    "        out_img = prev_img.squeeze(0).detach().cpu().numpy()\n",
    "        out_img = np.transpose(out_img, (1, 2, 0))\n",
    "        im.set_data(out_img)\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "    model_dir = os.path.join(Config.img_dir, Config.model_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "    save_path = os.path.join(model_dir, f\"sample_{sample_name}.png\")\n",
    "    plt.imsave(save_path, np.clip(out_img, 0, 1))\n",
    "\n",
    "    target = np.transpose(next_img.squeeze(0).detach().cpu().numpy(), (1, 2, 0))\n",
    "    save_path = os.path.join(model_dir, f\"sample_{sample_name}_target.png\")\n",
    "    plt.imsave(save_path, np.clip(target, 0, 1))\n",
    "    base = np.transpose(base_img.squeeze(0).detach().cpu().numpy(), (1, 2, 0))\n",
    "    save_path = os.path.join(model_dir, f\"sample_{sample_name}_base.png\")\n",
    "    plt.imsave(save_path, np.clip(base, 0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb1aa2",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from config import Config\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_conditioned=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        self.is_conditioned = is_conditioned\n",
    "        if is_conditioned:\n",
    "            self.condition = nn.Linear(Config.movement_embedding_dim + Config.latent_dimension + Config.time_embedding_dim, out_channels*2)\n",
    "        #self.memory_conditioning = nn.Linear(Config.movement_embedding_dim + Config.latent_dimension, out_channels)\n",
    "        #self.time_conditioning = nn.Linear(Config.time_embedding_dim, out_channels)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "    \n",
    "    def forward(self, x, conditioning=None):\n",
    "        fx = self.relu(self.bn1(self.conv1(x))) # test no batchnorm\n",
    "        fx = self.bn2(self.conv2(fx))\n",
    "\n",
    "        if self.is_conditioned:\n",
    "            gamma_beta = self.condition(conditioning)\n",
    "            gamma, beta = gamma_beta.chunk(2, dim=1)\n",
    "            gamma_scaled = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "            beta_scaled = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "            #Test doing a layer/groupnorm here before we modulate to be more like adaGN.\n",
    "            fx = fx * gamma_scaled + beta_scaled\n",
    "\n",
    "        hx = self.relu(fx + self.shortcut(x))\n",
    "        return hx\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        self.condition = nn.Linear(Config.movement_embedding_dim + Config.latent_dimension + Config.time_embedding_dim, out_channels*2)\n",
    "        #self.memory_conditioning = nn.Linear(Config.movement_embedding_dim + Config.latent_dimension, out_channels)\n",
    "        #self.time_conditioning = nn.Linear(Config.time_embedding_dim, out_channels)\n",
    "    \n",
    "    def forward(self, x, conditioning):\n",
    "        fx = self.relu(self.bn1(self.conv1(x))) # test no batchnorm\n",
    "        fx = self.bn2(self.conv2(fx))\n",
    "\n",
    "        gamma_beta = self.condition(conditioning)\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=1)\n",
    "        gamma_scaled = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta_scaled = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "        #Test doing a layer/groupnorm here before we modulate to be more like adaGN.\n",
    "        fx = fx * gamma_scaled + beta_scaled\n",
    "\n",
    "        hx = self.relu(fx)\n",
    "        return hx\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        #Give the dynamics the current timestep too - integer, not the float 0-1.\n",
    "        super().__init__()\n",
    "        features = Config.features\n",
    "        layers = []\n",
    "        for feature in features:\n",
    "            layers.append(ResBlock(in_channels, feature, is_conditioned=False))\n",
    "            layers.append(nn.Conv2d(feature, feature, kernel_size=3, stride=2, padding=1))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.down = nn.Sequential(*layers)\n",
    "\n",
    "        self.project_img = nn.Linear(features[-1], Config.latent_dimension)\n",
    "        self.predict = nn.Linear(Config.latent_dimension*2, Config.latent_dimension)\n",
    "\n",
    "    def forward(self, img, prev_state):\n",
    "        latent_img = self.down(img).squeeze(-1).squeeze(-1)\n",
    "        latent_img = self.project_img(latent_img)\n",
    "        state = torch.cat((latent_img, prev_state), dim=1)\n",
    "        new_state = self.predict(state)\n",
    "        return new_state\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        features = Config.features\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.strides = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.downs.append(ResBlock(in_channels, feature))\n",
    "            self.strides.append(nn.Conv2d(feature, feature, kernel_size=3, stride=2, padding=1))\n",
    "            in_channels = feature\n",
    "        \n",
    "        self.bottleneck = ResBlock(features[-1], features[-1]*2)\n",
    "        \n",
    "        self.ups = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        rev_features = features[::-1]\n",
    "        for feature in rev_features:\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.up_convs.append(ConvBlock(feature*2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "        self.embed_movement = nn.Linear(6, Config.movement_embedding_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x, t, m, l_emb):\n",
    "        skip_connections = []\n",
    "\n",
    "        #Embedding time step\n",
    "        t = t.unsqueeze(-1) #[num_dim, 1]\n",
    "        half_time_dim = Config.time_embedding_dim // 2\n",
    "        freq_exponents = torch.arange(half_time_dim, dtype=torch.float32, device=t.device)\n",
    "        freq_exponents = -math.log(10000) * freq_exponents / half_time_dim\n",
    "        freqs = torch.exp(freq_exponents) \n",
    "        freqs = freqs.unsqueeze(0)\n",
    "        angles = t * freqs  \n",
    "        t_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "\n",
    "        m_emb = self.embed_movement(m)\n",
    "        conditioning_emb = torch.cat([m_emb, l_emb, t_emb], dim=1)\n",
    "\n",
    "        for down, stride in zip(self.downs, self.strides):\n",
    "            x = down(x, conditioning_emb)\n",
    "            skip_connections.append(x)\n",
    "            x = stride(x)\n",
    "\n",
    "            #Maybe do conditioning here\n",
    "        \n",
    "        x = self.bottleneck(x, conditioning_emb) # add transformer layers here.\n",
    "        \n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(len(self.ups)):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
    "            \n",
    "            x = torch.cat([skip_connection, x], dim=1)\n",
    "            x = self.up_convs[idx](x, conditioning_emb)\n",
    "        \n",
    "        return self.final_conv(x) #delta predictions in each channel.\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = UNet()\n",
    "        self.dynamics = Dynamics()\n",
    "    \n",
    "    def predict_delta(self, x, t, m, l_emb):\n",
    "        return self.backbone(x, t, m, l_emb)\n",
    "\n",
    "    def predict_dynamics(self, img, prev_state):\n",
    "        return self.dynamics(img, prev_state)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return WorldModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1aa4b",
   "metadata": {},
   "source": [
    "## simulator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "from config import Config\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_vector(x, y, w, h):\n",
    "    nx = (2.0 * x - w) / w\n",
    "    ny = (h - 2.0 * y) / h\n",
    "    # project to unit sphere\n",
    "    length2 = nx*nx + ny*ny\n",
    "    if length2 > 1.0:\n",
    "        norm = 1.0 / np.sqrt(length2)\n",
    "        return np.array([nx*norm, ny*norm, 0.0])\n",
    "    else:\n",
    "        z = np.sqrt(1.0 - length2)\n",
    "        return np.array([nx, ny, z])\n",
    "\n",
    "class StreetViewTab:\n",
    "    def __init__(self, id, page):\n",
    "        self.id = id\n",
    "        self.page = page\n",
    "\n",
    "    async def take_screenshot(self):\n",
    "        screenshot_bytes = await self.page.screenshot(timeout=180_000)\n",
    "        image = Image.open(io.BytesIO(screenshot_bytes)).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(Config.model_resolution),    \n",
    "            transforms.ToTensor(),             # Convert to tensor\n",
    "            # transforms.Normalize(              # Normalize tensor\n",
    "            #     mean=[0.485, 0.456, 0.406],\n",
    "            #     std=[0.229, 0.224, 0.225]\n",
    "            # )\n",
    "        ])\n",
    "\n",
    "        tensor = transform(image)\n",
    "        return tensor\n",
    "\n",
    "    async def move(self):\n",
    "        await self.page.wait_for_selector('canvas.aFsglc', timeout=180_000)   \n",
    "        element = self.page.locator('canvas.aFsglc').first\n",
    "        box = await element.bounding_box()\n",
    "        if not box:\n",
    "            return\n",
    "\n",
    "        async def sample_point():\n",
    "            \"\"\"Keep sampling until point is on canvas.aFsglc.\"\"\"\n",
    "            while True:\n",
    "                rx = random.random()\n",
    "                ry = random.random()\n",
    "                x = box['x'] + rx * box['width']\n",
    "                y = box['y'] + ry * box['height']\n",
    "                hit = await self.page.evaluate(\n",
    "                    \"\"\"([x, y]) => document.elementFromPoint(x, y)?.className || null\"\"\",\n",
    "                    [x, y]\n",
    "                )\n",
    "                if hit and \"aFsglc\" in hit:\n",
    "                    return x, y, rx, ry\n",
    "\n",
    "        is_rotation = random.random() < Config.rotation_probability\n",
    "        if is_rotation:\n",
    "            # Pick two valid points\n",
    "            x1, y1, _, _ = await sample_point()\n",
    "            x2, y2, _, _ = await sample_point()\n",
    "\n",
    "            await self.page.mouse.move(x1, y1)\n",
    "            await self.page.mouse.down(button=\"left\")\n",
    "            await self.page.mouse.move(x2, y2, steps=15)\n",
    "            await self.page.mouse.up(button=\"left\")\n",
    "            await asyncio.sleep(4)\n",
    "\n",
    "            # Convert to vectors relative to box\n",
    "            v1 = convert_to_vector(x1 - box['x'], y1 - box['y'], box['width'], box['height'])\n",
    "            v2 = convert_to_vector(x2 - box['x'], y2 - box['y'], box['width'], box['height'])\n",
    "            axis = np.cross(v1, v2)\n",
    "            axis /= np.linalg.norm(axis)\n",
    "            dot = np.clip(np.dot(v1, v2), -1.0, 1.0)\n",
    "            theta = np.arccos(dot)\n",
    "            wq = np.cos(theta / 2.0)\n",
    "            s = np.sin(theta / 2.0)\n",
    "            xq, yq, zq = axis * s\n",
    "            return [wq, xq, yq, zq, 0, 0]\n",
    "\n",
    "        else:\n",
    "            x, y, rx, ry = await sample_point()\n",
    "            await self.page.mouse.move(x, y)\n",
    "            await self.page.mouse.click(x, y)\n",
    "            await asyncio.sleep(4)\n",
    "            return [1, 0, 0, 0, rx - 0.5, ry - 0.5]\n",
    "\n",
    "class Simulator():\n",
    "    def __init__(self):\n",
    "        self.initial_pages = Config.initial_pages\n",
    "        self.browser = None\n",
    "        self.context = None\n",
    "        self.playwright = None\n",
    "\n",
    "    async def setup(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=False)\n",
    "        self.context = await self.browser.new_context()\n",
    "\n",
    "        pages = await asyncio.gather(*(self.context.new_page() for _ in self.initial_pages))\n",
    "        self.tabs = [StreetViewTab(i, page) for i, page in enumerate(pages)]\n",
    "        await asyncio.gather(*(tab.page.goto(self.initial_pages[i]) for i, tab in enumerate(self.tabs)))\n",
    "        print(\"All tabs loaded\")\n",
    "\n",
    "    async def close(self):\n",
    "        await self.context.close()\n",
    "        await self.browser.close()\n",
    "        await self.playwright.stop()\n",
    "\n",
    "    async def get_images(self):\n",
    "        images_list = await asyncio.gather(*(tab.take_screenshot() for tab in self.tabs))\n",
    "        images_tensor = torch.stack(images_list) #[page_num, 3, h, w]\n",
    "        return images_tensor\n",
    "    \n",
    "    async def move(self):\n",
    "        move_list = await asyncio.gather(*(tab.move() for tab in self.tabs))\n",
    "        movement_tensor = torch.tensor(move_list)\n",
    "        return movement_tensor #[page_num, 6]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff704c",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from model import WorldModel\n",
    "from train import Trainer\n",
    "from simulator import Simulator\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        model = WorldModel()\n",
    "        simulator = Simulator()\n",
    "        await simulator.setup()\n",
    "        trainer = Trainer(model, simulator)\n",
    "        if Config.load_model:\n",
    "            trainer.load_checkpoint()\n",
    "        await trainer.train()\n",
    "    finally:\n",
    "        await simulator.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
